{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Italian corpus is from this source: https://www.corpusitaliano.it/en/contents/description.html\n",
    "\n",
    "It is the corpus that contains 220 million words from online material. \n",
    "\n",
    "They provide: \n",
    "1. the corpus\n",
    "2. the annotated corpus\n",
    "3. the occurrence counts for all the element\n",
    "4. the occurrence counts for the elements without numbers etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir=\"/home/tlei/Desktop/Italian_Corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the occurence file, get the frequence profile\n",
    "IT_lemma_freq=pd.read_csv(os.path.join(basedir,'lemma-WITHOUTnumberssymbols-frequencies-paisa.txt'), sep=\",\", header=1, names=[\"lemma\",\"occurrence\"])\n",
    "total_lemma_counts=IT_lemma_freq['occurrence'].sum()\n",
    "IT_lemma_freq[\"lemma\"]=IT_lemma_freq.lemma.astype(str)\n",
    "IT_lemma_freq[\"freq\"]=IT_lemma_freq.occurrence.apply(lambda x: round(10e6*x/total_lemma_counts,2)).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the criteria to the words to get the lists\n",
    "# word length 3,4,5,6,7 \n",
    "word_length=IT_lemma_freq['lemma'].apply(lambda x: (len(x) >= 4) and len(x)<=7)\n",
    "# filter the high and low frequency word from the list\n",
    "# standard high freq occurrence> 1000 \n",
    "high_freq=IT_lemma_freq['freq'].apply(lambda x: x>1000)\n",
    "# standard low freq  occuance> 1  occurrence <39\n",
    "low_freq=IT_lemma_freq['freq'].apply(lambda x: x<39 and x>1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_high_freq=IT_lemma_freq[word_length & high_freq]\n",
    "IT_low_freq=IT_lemma_freq[word_length & low_freq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the annotated corpus to get the linguistic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 225292817 words.\n"
     ]
    }
   ],
   "source": [
    "# Some checks\n",
    "# count how many words are in the corpus\n",
    "# Define a function to count words\n",
    "def count_words_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()  # Read the content of the file\n",
    "        words = text.split()  # Split the text into words\n",
    "        word_count = len(words)  # Count the number of words\n",
    "    return word_count\n",
    "\n",
    "# Specify the path to your corpus file\n",
    "file_path = '/Users/tiger/italian_stim/corpus/paisa.raw.utf8'\n",
    "\n",
    "# Count words\n",
    "word_count = count_words_in_file(file_path)\n",
    "print(f'The file contains {word_count} words.')\n",
    "\n",
    "## not vary a lot, we will use the occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_high_freq = IT_high_freq.rename(columns={'occurance': 'freq_per_mil'})\n",
    "IT_high_freq['freq_per_mil']=IT_high_freq['freq_per_mil'].apply(lambda x: round(10e6*x/total_lemma_counts,2))\n",
    "IT_low_freq = IT_low_freq.rename(columns={'occurance': 'freq_per_mil'})\n",
    "IT_low_freq['freq_per_mil']=IT_low_freq['freq_per_mil'].apply(lambda x: round(10e6*x/total_lemma_counts,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID       FORM      LEMMA CPOSTAG POSTAG                    FEATS  \\\n",
      "0          1         La         il       R     RD              num=s|gen=f   \n",
      "1          2      Siria      Siria       S     SP                        _   \n",
      "2          3          Ã¨     essere       V      V  num=s|per=3|mod=i|ten=p   \n",
      "3          4         un         un       R     RI              num=s|gen=m   \n",
      "4          5  obiettivo  obiettivo       S      S              num=s|gen=m   \n",
      "...       ..        ...        ...     ...    ...                      ...   \n",
      "22103828  11         il         il       R     RD              num=s|gen=m   \n",
      "22103829  12          6          6       N      N                        _   \n",
      "22103830  13     agosto     agosto       S      S              num=s|gen=m   \n",
      "22103831  14       2010       2010       N      N                        _   \n",
      "22103832  15          .          .       F     FS                        _   \n",
      "\n",
      "         HEAD    DEPREL  \n",
      "0           2       det  \n",
      "1           3      subj  \n",
      "2           0      ROOT  \n",
      "3           5       det  \n",
      "4           3      pred  \n",
      "...       ...       ...  \n",
      "22103828   12       det  \n",
      "22103829    8  mod_temp  \n",
      "22103830   12       mod  \n",
      "22103831   13       mod  \n",
      "22103832    8      punc  \n",
      "\n",
      "[22103833 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# make Italian check list, to have the lexical infomation of each words so that we can select\n",
    "\n",
    "\n",
    "def parse_paisa_corpus(file_path):\n",
    "    # Define the columns based on the provided description\n",
    "    columns = [\n",
    "        'ID', 'FORM', 'LEMMA', 'CPOSTAG', 'POSTAG', \n",
    "        'FEATS', 'HEAD', 'DEPREL', 'unused_1', 'unused_2'\n",
    "    ]\n",
    "    \n",
    "    # Initialize an empty list to store rows of the DataFrame\n",
    "    data = []\n",
    "    \n",
    "    # Read the file and parse it\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Skip comment lines or empty lines\n",
    "            if line.startswith('#') or line.strip() == \"\":\n",
    "                continue\n",
    "                \n",
    "            # Split the line into fields based on whitespace\n",
    "            fields = line.split('\\t')\n",
    "            \n",
    "            # Make sure the line has 10 fields (to match the structure)\n",
    "            if len(fields) == 10:\n",
    "                # Append only relevant columns and drop 'unused_1' and 'unused_2'\n",
    "                data.append(fields[:8])  # We take only the first 8 fields\n",
    "                \n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(data, columns=columns[:8])  # Use only relevant column names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "file_path =  '/home/tlei/Desktop/paisa.annotated.CoNLL.utf8'\n",
    "df = parse_paisa_corpus(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "votcloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
